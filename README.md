### Implemented a Neural Netwrok from scratch with RMSprop
* Network architecture:   
Input layer    
Dense hidden layer with 512 neurons, using relu as the activation function  
Dropout with a value of 0.2  
Dense hidden layer with 512 neurons, using relu as the activation function  
Dropout with a value of 0.2  
Output layer, using softmax as the activation function    

Achieved testing accuarcy of 84 % on Fashion MNIST dataset

